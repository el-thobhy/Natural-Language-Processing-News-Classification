# -*- coding: utf-8 -*-
"""Membuat Model NLP dengan TensorFlow.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1TP7rj5h7hDthxIS_FWki5R7-vksMHJ22

# **Proyek Pertama** : Membuat Model NLP dengan TensorFlow

Nama: Auriwan Yasper

email: auriwanyasper@gmail.com

dataset yang digunakan adalah sebagai berikut

https://www.kaggle.com/kishanyadav/inshort-news?select=inshort_news_data-1.csv
"""

#mengupload dataset
from google.colab import files
uploaded = files.upload()

#extrax file zip ke folder baru
import zipfile

local_zip='/content/inshort_news_data-1.csv.zip'
zip_ref=zipfile.ZipFile(local_zip,'r')
zip_ref.extractall('/dataset_baru')
zip_ref.close()

#membuat dataframe dari dataset
import pandas as pd
df=pd.read_csv('/dataset_baru/inshort_news_data-1.csv')
df.info()

#pilih data dengan menhapus data tidak penting
df=df.drop(columns=['Unnamed: 0','news_headline'])
df

"""# **Pengolahan data**
data diolah mejadi lebih ringkas dalam huruf kecil, tampa kata-kata kurang penting dan tanda baca agar pelatihan lebih maksimal dan bisa diproses oleh tokenizer
"""

#melihat data text
df.news_article

#ubah data jadi huruf kecil semua
df = df.applymap(lambda s: s.lower() if type(s) == str else s)
df

# Menghapus tanda baca
import string
def haspusTandaBaca(text):
  return(text.translate(str.maketrans(' ',' ',string.punctuation)))

df.news_article = df.news_article.apply(haspusTandaBaca)
df

# Menghapus kata  dalam news_article yang tidak memiliki makna dengan stopwords
import nltk
from nltk.corpus import stopwords
nltk.download('stopwords')

st_words = stopwords.words()
def cekKata(text):
  return(' '.join([w for w in text.split() if w not in st_words]))
  
df.news_article = df.news_article.apply(lambda x: cekKata(x))
df.head()

#cek data kategori
df.news_category.value_counts()

#cek data artikel
df.news_article

"""**melakukan one-hot encoding data ketegorical**

lalu membuat data_baru
"""

#one-hot encoding data kategorikal
category=pd.get_dummies(df.news_category)
df_baru=pd.concat([df,category],axis=1)
df_baru=df_baru.drop(columns='news_category')
df_baru

"""Agar dapat diproses oleh model, kita perlu mengubah nilai-nilai dari dataframe ke dalam tipe data numpy array menggunakan atribut values"""

#mengubah dataframe menjadi numpy data array
artikel=df_baru['news_article'].values
label=df_baru[['world','entertainment','sports','technology','politics','science','automobile'
               ]].values

#cek data text array
artikel

"""Membagi data menjadi data training dan testing"""

#melakukan split data menjadi data training dan validation
from sklearn.model_selection import train_test_split
artikel_latih, artikel_test, label_latih, label_test = train_test_split(artikel, label, test_size=0.2)

"""mengubah setiap kata pada dataset kita ke dalam bilangan numerik dengan fungsi Tokenizer. Setelah tokenisasi selesai, kita perlu membuat mengonversi setiap sampel menjadi sequence."""

#mengubah data text menjadi bilangan numerik dengan fungsi tokenizer
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

tokenizer = Tokenizer(num_words=5000, oov_token='x')
tokenizer.fit_on_texts(artikel_latih)
tokenizer.fit_on_sequences(artikel_test)

#membuat sequence dari setiap data yang sudah diubah
sekuens_latih = tokenizer.texts_to_sequences(artikel_latih)
sekuens_test = tokenizer.texts_to_sequences(artikel_test)

#menambahkan padding pada kalimat dengan pad sequences agar kalimat lbih seragam
paded_latih = pad_sequences(sekuens_latih)
paded_test = pad_sequences(sekuens_test)

"""arsitektur model menggunakan layer Embedding dengan dimensi embedding sebesar 64, serta dimensi dari input sebesar nilai jumlah kata pada objek tokenizer"""

#cek jumlah data dalam text dengan menggunakan tokenizer.word_text
a = tokenizer.word_index
jumlah_vocab = len(a)
jumlah_vocab

#membuat model dengan sequential dengan layer Embedding dan Arsitektur model LSTM
import tensorflow as tf
model = tf.keras.Sequential([
        tf.keras.layers.Embedding(input_dim=jumlah_vocab, output_dim=64),
        tf.keras.layers.LSTM(128),
        tf.keras.layers.Dense(128, activation='relu'),
        tf.keras.layers.Dropout(0.5),
        tf.keras.layers.Dense(7, activation='softmax')
])

#melihat ringkasan model
model.summary()

"""Membuat Callback class untuk fungsi henti jika akurasi model sudah melebihi 90%"""

#callback
class callbackDulu(tf.keras.callbacks.Callback):
  def on_epoch_end(self, epoch, logs={}):
    if(logs.get('val_accuracy')>=0.90 and logs.get('accuracy')>=0.90):
      print("\nAkurasi model sudah melebihi 90%")
      self.model.stop_training =True
henti=callbackDulu()

"""Selanjutnya mengcompile model dan menententukan optimizer serta loss function yang akan dipakai oleh model"""

#compile model
model.compile(
    loss='categorical_crossentropy',
    optimizer='adam',
    metrics=['accuracy'])

"""melatih model dengan memanggil fungsi fit()."""

#training model dengan data yang sudah
history = model.fit(
    paded_latih, label_latih, 
    batch_size=32,
    validation_split=0.2,
    epochs=30, 
    validation_data=(paded_test,label_test),
    verbose=2,
    callbacks=[henti]
)

"""Membuat plot loss dan akurasi pada saat training dan validation."""

# Plot akurasi
import matplotlib.pyplot as plt
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Model Accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='lower right')
plt.show()

# Plot loss
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model Loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper right')
plt.show()